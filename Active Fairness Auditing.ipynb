{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "def encoding_categorical_variables(data):\n",
    "\n",
    "    all_cols = data.columns.tolist()\n",
    "\n",
    "    encoder = OrdinalEncoder(categories=[['at_home', 'teacher', 'health','services','other']])\n",
    "    data[['Mjob']] = encoder.fit_transform(data[['Mjob']]).astype('int64')\n",
    "    data[['Fjob']] = encoder.fit_transform(data[['Fjob']]).astype('int64')\n",
    "\n",
    "    encoder = OrdinalEncoder(categories=[['other', 'mother', 'father']])\n",
    "    data[['guardian']] = encoder.fit_transform(data[['guardian']]).astype('int64')\n",
    "\n",
    "    encoder = OrdinalEncoder(categories=[['home', 'reputation', 'course', 'other']])\n",
    "    data[['reason']] = encoder.fit_transform(data[['reason']]).astype('int64')\n",
    "\n",
    "    encoder = OrdinalEncoder(categories=[['no', 'yes']])\n",
    "    data[['schoolsup']] = encoder.fit_transform(data[['schoolsup']]).astype('int64')\n",
    "    data[['famsup']] = encoder.fit_transform(data[['famsup']]).astype('int64')\n",
    "    data[['paid']] = encoder.fit_transform(data[['paid']]).astype('int64')\n",
    "    data[['activities']] = encoder.fit_transform(data[['activities']]).astype('int64')\n",
    "    data[['nursery']] = encoder.fit_transform(data[['nursery']]).astype('int64')\n",
    "    data[['higher']] = encoder.fit_transform(data[['higher']]).astype('int64')\n",
    "    data[['internet']] = encoder.fit_transform(data[['internet']]).astype('int64')\n",
    "    data[['romantic']] = encoder.fit_transform(data[['romantic']]).astype('int64')\n",
    "\n",
    "    categorical_cols = ['school', 'sex', 'address', 'famsize', 'Pstatus']\n",
    "    categorical_data = data[categorical_cols]\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_data = encoder.fit_transform(categorical_data).astype('int64')\n",
    "\n",
    "    data_encoded = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    numeric_cols = [col for col in all_cols if col not in categorical_cols]\n",
    "\n",
    "    data = pd.concat([data_encoded, data[numeric_cols]], axis=1)\n",
    "\n",
    "    # Droping column 'age' as G3 scores are independent of 'age'\n",
    "    data.drop('age', axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def target_variable_encoding(data, data_test):\n",
    "\n",
    "  # For binary classification tasks, according to the research paper, pass is considered if G3 >= 10, else fail\n",
    "    for index, row in data.iterrows():\n",
    "        if row['G3'] >= 13:\n",
    "            data.at[index, 'G3'] = 1\n",
    "        else:\n",
    "            data.at[index, 'G3'] = 0\n",
    "\n",
    "    for index, row in data_test.iterrows():\n",
    "        if row['G3'] >= 13:\n",
    "            data_test.at[index, 'G3'] = 1\n",
    "        else:\n",
    "            data_test.at[index, 'G3'] = 0\n",
    "\n",
    "    return [data, data_test]\n",
    "\n",
    "def test_train_split(data):\n",
    "    # Out of 649 samples, 120 random samples are considered for the test set, and remaining ones for training.\n",
    "    test_data_indices = random.sample(range(650), 120)\n",
    "\n",
    "    data_test = data.iloc[test_data_indices].copy()\n",
    "    data.drop(index=test_data_indices, inplace=True)\n",
    "\n",
    "    data_test.reset_index(drop=True, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return [data, data_test]\n",
    "\n",
    "def feature_scaling(data, data_test):\n",
    "\n",
    "  # All these columns are considered for feature scaling, to bring their value between 0 & 1\n",
    "    feature_scaling_cols = ['Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures',\n",
    "                          'famrel','freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    data[feature_scaling_cols] = scaler.fit_transform(data[feature_scaling_cols])\n",
    "\n",
    "    data_test[feature_scaling_cols] = scaler.transform(data_test[feature_scaling_cols])\n",
    "\n",
    "    return [data, data_test]\n",
    "\n",
    "def train_test_split_arrays(data, data_test):\n",
    "\n",
    "    X_train = data.iloc[:, :-1].values\n",
    "\n",
    "    # The target variable variable 'G3' is considered as 'y'\n",
    "    y_train = data.iloc[:, -1:].values\n",
    "\n",
    "\n",
    "    X_test = data_test.iloc[:, :-1].values\n",
    "    y_test = data_test.iloc[:, -1:].values\n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('student-por.csv', sep=\",\")\n",
    "data = encoding_categorical_variables(data)\n",
    "data, data_test = test_train_split(data)\n",
    "data, data_test = feature_scaling(data, data_test)\n",
    "data, data_test = target_variable_encoding(data, data_test)\n",
    "x_train, y_train, x_test, y_test = train_test_split_arrays(data, data_test)\n",
    "x = np.vstack((x_train, x_test))\n",
    "y = np.vstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get max Delta(x,h) such that h is consistent with h_hat on T\n",
    "def find_h1(h_hat, T, x):\n",
    "\n",
    "    lag_terms = []\n",
    "    lambs = np.linspace(0,40,num=100)\n",
    "    present_model = None\n",
    "\n",
    "    for j in range(len(lambs)):\n",
    "        new_model = LogisticRegression(max_iter = 500)\n",
    "        yi = []\n",
    "        lamb = lambs[j]\n",
    "        weights = []\n",
    "\n",
    "        g0 = g1 = 0\n",
    "        for i in range(len(x)):\n",
    "            g0 += x[i,2]== 0\n",
    "            g1 += x[i,2]==1\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            Ci0 = (x[i,2] == 1) * 0 + (x[i,2] == 0) * (-1/g0)\n",
    "            Ci1 = (x[i,2] == 1) * (-1/g1) + (x[i,2] == 0) * 0\n",
    "\n",
    "            if i in T:\n",
    "                Ci0 += -lamb * (h_hat.predict(x[i].reshape(1,-1))[0] == 0)\n",
    "                Ci1 += -lamb * (h_hat.predict(x[i].reshape(1,-1))[0] == 1)\n",
    "\n",
    "            weights.append(np.abs(Ci0 - Ci1))\n",
    "            yi.append(Ci0 >= Ci1)\n",
    "\n",
    "        new_model.fit(x, yi,sample_weight=weights)\n",
    "        present_model = new_model\n",
    "\n",
    "        lag_term = 0\n",
    "        for s in T:\n",
    "            lag_term += new_model.predict(x[s].reshape(1,-1)) == h_hat.predict(x[s].reshape(1,-1))\n",
    "        lag_terms.append(lag_term)\n",
    "        if(lag_term == len(T)):\n",
    "            break\n",
    "\n",
    "    return present_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get min Delta(x,h) such that h is consistent with h_hat on T\n",
    "def find_h2(h_hat, T, x):\n",
    "\n",
    "    lag_terms = []\n",
    "    lambs = np.linspace(0,40,num=100)\n",
    "    present_model = None\n",
    "\n",
    "    for j in range(len(lambs)):\n",
    "        new_model = LogisticRegression(max_iter=500)\n",
    "        yi = []\n",
    "        lamb = lambs[j]\n",
    "        weights = []\n",
    "        g0 = g1 = 0\n",
    "        for i in range(len(x)):\n",
    "            g0 += x[i,2]== 0\n",
    "            g1 += x[i,2]==1\n",
    "        for i in range(len(x)):\n",
    "            Ci0 = (x[i,2] == 1) * 0 + (x[i,2] == 0) * (1/g0)\n",
    "            Ci1 = (x[i,2] == 1) * (1/g1) + (x[i,2] == 0) * 0\n",
    "\n",
    "            if i in T:\n",
    "                Ci0 += -lamb * (h_hat.predict(x[i].reshape(1,-1))[0] == 0)\n",
    "                Ci1 += -lamb * (h_hat.predict(x[i].reshape(1,-1))[0] == 1)\n",
    "\n",
    "            weights.append(np.abs(Ci0 - Ci1))\n",
    "            yi.append(Ci0 >= Ci1)\n",
    "\n",
    "        new_model.fit(x, yi,sample_weight=weights)\n",
    "        present_model = new_model\n",
    "        lag_term = 0\n",
    "        for s in T:\n",
    "            lag_term += new_model.predict(x[s].reshape(1,-1)) == h_hat.predict(x[s].reshape(1,-1))\n",
    "        lag_terms.append(lag_term)\n",
    "\n",
    "        if(lag_term == len(T)):\n",
    "            break\n",
    "\n",
    "    return present_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_learning_oracle(X,predictions,S):\n",
    "    model = LogisticRegression()\n",
    "    y = []\n",
    "    for i in S:\n",
    "        y.append(predictions[i])\n",
    "    y = np.array(y)\n",
    "    if(len(S) > 0):\n",
    "        model.fit(X[S], y.ravel())\n",
    "    else:\n",
    "        y = np.array([0,1]).reshape(-1,1)\n",
    "        model.fit(X[[0,1]], y.ravel())\n",
    "    return model\n",
    "\n",
    "def mu(model, X):\n",
    "    num_0 = 0\n",
    "    num_1 = 0\n",
    "    sum_0 = 0\n",
    "    sum_1 = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if(X[i,2]):\n",
    "            num_1 += 1\n",
    "            sum_1 += model.predict(X[i].reshape(1,-1))[0] == 1\n",
    "\n",
    "        else:\n",
    "            num_0 += 1\n",
    "            sum_0 += model.predict(X[i].reshape(1,-1))[0] == 1\n",
    "\n",
    "    if(num_0 > 0 and num_1 > 0): return sum_1/num_1 - sum_0/num_0\n",
    "    elif(num_0 == 0): return sum_1/num_1\n",
    "    return sum_0/num_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Active Fairness Auditing Algorithm\n",
    "def active_auditing(X,M,model,epsilon,budget):\n",
    "    \n",
    "    S = set()\n",
    "    predictions = dict()\n",
    "    budget_used = 0\n",
    "\n",
    "    while True:\n",
    "        w = [1/len(X) for _ in range(len(X))]\n",
    "        fail_prob = 0.1\n",
    "        tau_x = [np.random.exponential(1/(2*np.log(X.shape[1]) + np.log(M) - np.log(fail_prob))) for _ in range(len(X))]\n",
    "        h_hat = online_learning_oracle(X,predictions,list(S))  \n",
    "        T = set()\n",
    "\n",
    "        while True:\n",
    "            h1 = find_h1(h_hat, T, X)\n",
    "            h2 = find_h2(h_hat, T, X)\n",
    "\n",
    "            mu_diff = mu(h1,X) - mu(h2,X)\n",
    "  \n",
    "            if mu_diff <= 2 * epsilon:\n",
    "                break  \n",
    "\n",
    "            delta = {i for i in range(len(X)) if h1.predict(X[i].reshape(1,-1)) != h_hat.predict(X[i].reshape(1,-1)) or h2.predict(X[i].reshape(1,-1)) != h_hat.predict(X[i].reshape(1,-1))}\n",
    "\n",
    "            while sum(w[i] for i in delta) <= 1 + 1e-3:\n",
    "                for i in delta:\n",
    "                    w[i] *= 2  \n",
    "                T = set([i for i in range(len(x)) if w[i] >= tau_x[i]])\n",
    "\n",
    "        diff = 0\n",
    "        for el in T:\n",
    "            diff += h_hat.predict(X[el].reshape(1,-1)) != model.predict(X[el].reshape(1,-1))\n",
    "\n",
    "        \n",
    "        for i in T:\n",
    "            if(budget_used >= budget): break\n",
    "            predictions[i] = model.predict(X[i].reshape(1,-1))[0]\n",
    "            S.add(i)\n",
    "            budget_used += 1\n",
    "\n",
    "        if(budget_used >= budget):\n",
    "            return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this multiple times to get the mean value of the deviations\n",
    "budgets = 50\n",
    "M = 100\n",
    "epsilon = 0.25\n",
    "u_D = np.abs(mu(model, x))\n",
    "\n",
    "S = list(active_auditing(x, M, model, epsilon, budget))\n",
    "u1 = np.abs(mu(find_h1(model, S, x), x))\n",
    "u2 = np.abs(mu(find_h2(model, S, x), x))\n",
    "u = max(u1, u2)\n",
    "u_S = np.abs(mu(model, x[S]))\n",
    "print(max(u - u_D, 0))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
